{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_submission (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr-qPGoRVOSH",
        "outputId": "add725a4-894d-482d-e002-5927bc4c47d9"
      },
      "source": [
        "#adaboost\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "f1 = make_scorer(f1_score,average='macro')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for column in df.columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        result[column] = (df[column] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "train_labels = train_labels.iloc[:320,:]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "fscore_list = []\n",
        "for i in range(0,3):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "\n",
        "  X_train = X_train.iloc[:,1:]\n",
        "  X_test = X_test.iloc[:,1:]\n",
        "  y_train = y_train.iloc[:,1:]\n",
        "  y_test = y_test.iloc[:,1:]\n",
        "\n",
        "  #normalization\n",
        "  X_train = normalize(X_train)\n",
        "  X_test = normalize(X_test)\n",
        "\n",
        "\n",
        "  # ada boost grid search to find best parameters\n",
        "\n",
        "  ada=AdaBoostClassifier()\n",
        "\n",
        "  param_grid={'n_estimators' : [5, 7, 10, 50, 100, 200, 250, 350, 500],\n",
        "              'learning_rate' : [0.0001,.001,0.01,.1]}\n",
        "\n",
        "  pca = PCA().fit(X_train)\n",
        "  comp_list = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_comp = np.where(comp_list == max(np.cumsum(pca.explained_variance_ratio_)))\n",
        "  n_comp = int(n_comp[0])\n",
        "\n",
        "\n",
        "  CV_ada = GridSearchCV(estimator = ada, param_grid = param_grid, scoring = f1, n_jobs = -1, cv = 5, verbose = 2)\n",
        "  CV_ada.fit(X_train, y_train)\n",
        "  CV_ada.best_params_\n",
        "  lr = CV_ada.best_params_['learning_rate']\n",
        "  n_est = CV_ada.best_params_['n_estimators']\n",
        "\n",
        "  Adaboostpipeline = Pipeline([('pca', PCA(n_components = n_comp)),\n",
        "                  ('Adaboost', AdaBoostClassifier(n_estimators=n_est, learning_rate=lr))])\n",
        "  Adaboost_model = AdaBoostClassifier(learning_rate = lr, n_estimators = n_est)\n",
        "  Adaboostpipeline.fit(X_train,y_train)\n",
        "  ada_pred = Adaboostpipeline.predict(X_test)\n",
        "  fscore = fbeta_score(ada_pred,y_test,1)\n",
        "  fscore_list.append(fscore)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # knn grid search to find best parameters\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# knn = KNeighborsClassifier()\n",
        "\n",
        "# param_grid={'n_neighbors' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
        "\n",
        "# CV_knn = GridSearchCV(estimator = knn, param_grid = param_grid, scoring = f1, n_jobs = -1, cv = 5, verbose = 3)\n",
        "\n",
        "# CV_knn.fit(X_train, y_train)\n",
        "\n",
        "# CV_knn.best_params_\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   12.7s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.4min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   21.4s\n",
            "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.4min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   21.9s\n",
            "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  1.4min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouCwL7aH5klV",
        "outputId": "55f43fc3-725a-4ac0-af91-70bdc054058d"
      },
      "source": [
        "# #adaboost\n",
        "# import statistics \n",
        "# print(fscore_list)\n",
        "# statistics.mean(fscore_list)\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "# train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "# train_labels = train_labels.iloc[:320,:]\n",
        "test_features=test_data.iloc[:,1:]\n",
        "test_class = test_labels.iloc[:,1:]\n",
        "test_features = normalize(test_features)\n",
        "print(test_features)\n",
        "print(test_class)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          1         2         3    ...       98        99        100\n",
            "320  0.066894  0.512442  0.279146  ...  0.748666  0.436748  0.329982\n",
            "321  0.000000  0.592382  0.299369  ...  0.462290  0.607292  0.274579\n",
            "322  0.072159  0.580489  0.285496  ...  0.651240  0.270081  0.688928\n",
            "323  0.411794  0.573735  0.242869  ...  0.731429  0.706134  0.523915\n",
            "324  0.229338  0.620514  0.402198  ...  0.349374  0.290856  0.601860\n",
            "..        ...       ...       ...  ...       ...       ...       ...\n",
            "395  0.433923  0.501028  0.210511  ...  0.630508  0.347569  0.515899\n",
            "396  0.350500  0.600911  0.328740  ...  0.425455  0.331308  0.607617\n",
            "397  0.611785  0.519453  0.203868  ...  0.581110  0.402373  0.603322\n",
            "398  0.243447  0.548269  0.277370  ...  0.618371  0.386227  0.560762\n",
            "399  0.729114  0.503180  0.200796  ...  0.622715  0.335880  0.571435\n",
            "\n",
            "[80 rows x 100 columns]\n",
            "     label\n",
            "320      1\n",
            "321      1\n",
            "322      1\n",
            "323      0\n",
            "324      1\n",
            "..     ...\n",
            "395      0\n",
            "396      1\n",
            "397      0\n",
            "398      1\n",
            "399      0\n",
            "\n",
            "[80 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR4r2s_gpSHQ",
        "outputId": "a3e9f7b1-0dd6-422f-a1e6-8a4cb1f9e39a"
      },
      "source": [
        "#knn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "f1 = make_scorer(f1_score,average='macro')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for column in df.columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        result[column] = (df[column] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "train_labels = train_labels.iloc[:320,:]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "fscore_knn_list = []\n",
        "for i in range(0,3):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "\n",
        "  X_train = X_train.iloc[:,1:]\n",
        "  X_test = X_test.iloc[:,1:]\n",
        "  y_train = y_train.iloc[:,1:]\n",
        "  y_test = y_test.iloc[:,1:]\n",
        "\n",
        "  #normalization\n",
        "  X_train = normalize(X_train)\n",
        "  X_test = normalize(X_test)\n",
        "\n",
        "\n",
        "  # ada boost grid search to find best parameters\n",
        "\n",
        "  knn = KNeighborsClassifier()\n",
        "\n",
        "  param_grid={'n_neighbors' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
        "\n",
        "  pca = PCA().fit(X_train)\n",
        "  comp_list = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_comp = np.where(comp_list == max(np.cumsum(pca.explained_variance_ratio_)))\n",
        "  n_comp = int(n_comp[0])\n",
        "\n",
        "\n",
        "  CV_knn = GridSearchCV(estimator = knn, param_grid = param_grid, scoring = f1, n_jobs = -1, cv = 5, verbose = 3)\n",
        "  CV_knn.fit(X_train, y_train)\n",
        "  CV_knn.best_params_\n",
        "  n_neigh = CV_knn.best_params_['n_neighbors']\n",
        "\n",
        "  knnpipeline = Pipeline([('pca', PCA(n_components = n_comp)),\n",
        "                  ('knn', KNeighborsClassifier(n_neighbors = n_neigh))])\n",
        "  knnpipeline.fit(X_train,y_train)\n",
        "  knn_pred = knnpipeline.predict(X_test)\n",
        "  fscore = fbeta_score(knn_pred,y_test,1)\n",
        "  fscore_knn_list.append(fscore)\n",
        "\n",
        "#average fscore calculation\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    0.9s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    0.8s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    0.8s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-vOIhIT91mk",
        "outputId": "1f17a26f-9f17-44ef-86f0-ec62375d72ce"
      },
      "source": [
        "import statistics \n",
        "print(fscore_knn_list)\n",
        "print(\"knn Fscore:\",statistics.mean(fscore_knn_list))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8080808080808082, 0.8958333333333333, 0.8514851485148515]\n",
            "knn Fscore: 0.8517997633096643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzuF7kox-BUV",
        "outputId": "26841c31-3c60-4b99-b450-10808d89b9fb"
      },
      "source": [
        "#SVM\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "f1 = make_scorer(f1_score,average='macro')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for column in df.columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        result[column] = (df[column] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "train_labels = train_labels.iloc[:320,:]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "fscore_svm_list = []\n",
        "for i in range(0,3):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "\n",
        "  X_train = X_train.iloc[:,1:]\n",
        "  X_test = X_test.iloc[:,1:]\n",
        "  y_train = y_train.iloc[:,1:]\n",
        "  y_test = y_test.iloc[:,1:]\n",
        "\n",
        "  #normalization\n",
        "  X_train = normalize(X_train)\n",
        "  X_test = normalize(X_test)\n",
        "\n",
        "\n",
        "  # ada boost grid search to find best parameters\n",
        "\n",
        "  svm = SVC()\n",
        "\n",
        "  param_grid = {'C': [0.001,0.01, 0.1, 1, 10],\n",
        "              'gamma' : [0.001,0.01, 0.1, 1],\n",
        "              'kernel' : ['linear','rbf']}\n",
        "\n",
        "  pca = PCA().fit(X_train)\n",
        "  comp_list = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_comp = np.where(comp_list == max(np.cumsum(pca.explained_variance_ratio_)))\n",
        "  n_comp = int(n_comp[0])\n",
        "\n",
        "\n",
        "  CV_svm = GridSearchCV(estimator = svm, param_grid = param_grid, scoring = f1, n_jobs = -1, cv = 5, verbose = 3)\n",
        "  CV_svm.fit(X_train, y_train)\n",
        "  CV_svm.best_params_\n",
        "  c = CV_svm.best_params_['C']\n",
        "  gam = CV_svm.best_params_['gamma']\n",
        "  ker = CV_svm.best_params_['kernel']\n",
        "  svmpipeline = Pipeline([('pca', PCA(n_components = n_comp)),\n",
        "                 ('SVM', SVC(C=c, gamma=gam,kernel=ker))])\n",
        "  svmpipeline.fit(X_train,y_train)\n",
        "  svm_pred = svmpipeline.predict(X_test)\n",
        "  fscore = fbeta_score(svm_pred,y_test,1)\n",
        "  fscore_svm_list.append(fscore)\n",
        "\n",
        "#average fscore calculation\n",
        "import statistics \n",
        "print(fscore_svm_list)\n",
        "print(\"svm Fscore:\",statistics.mean(fscore_svm_list))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    1.8s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    1.7s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.4788732394366197, 0.23076923076923078, 0.8043478260869567]\n",
            "svm Fscore: 0.5046634320976023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    1.6s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0b05BsO_nWW",
        "outputId": "0ddd13f1-3557-435e-ee18-a8b0d00de0e9"
      },
      "source": [
        "#GBM\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "f1 = make_scorer(f1_score,average='macro')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for column in df.columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        result[column] = (df[column] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "train_labels = train_labels.iloc[:320,:]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "fscore_gbm_list = []\n",
        "for i in range(0,3):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "\n",
        "  X_train = X_train.iloc[:,1:]\n",
        "  X_test = X_test.iloc[:,1:]\n",
        "  y_train = y_train.iloc[:,1:]\n",
        "  y_test = y_test.iloc[:,1:]\n",
        "\n",
        "  #normalization\n",
        "  X_train = normalize(X_train)\n",
        "  X_test = normalize(X_test)\n",
        "\n",
        "\n",
        "  # ada boost grid search to find best parameters\n",
        "\n",
        "  gbm = GradientBoostingClassifier()\n",
        "\n",
        "  param_grid = { 'learning_rate': [0.1,0.2,0.4,1.0],\n",
        "              'n_estimators' : [100,200,300,50],\n",
        "              'max_depth' : [3,5,20,40],\n",
        "              'max_features':['auto', 'sqrt', 'log2'],\n",
        "              'loss':['deviance', 'exponential']}\n",
        "\n",
        "  pca = PCA().fit(X_train)\n",
        "  comp_list = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_comp = np.where(comp_list == max(np.cumsum(pca.explained_variance_ratio_)))\n",
        "  n_comp = int(n_comp[0])\n",
        "\n",
        "\n",
        "  CV_gbm = GridSearchCV(estimator = gbm, param_grid = param_grid, scoring = f1, n_jobs = -1, cv = 5, verbose = 3)\n",
        "  CV_gbm.fit(X_train, y_train)\n",
        "  CV_gbm.best_params_\n",
        "  lr = CV_gbm.best_params_['learning_rate']\n",
        "  n_est = CV_gbm.best_params_['n_estimators']\n",
        "  dep = CV_gbm.best_params_['max_depth']\n",
        "  fea = CV_gbm.best_params_['max_features']\n",
        "  los = CV_gbm.best_params_['loss']\n",
        "\n",
        "  gbmpipeline = Pipeline([('pca', PCA(n_components = n_comp)),\n",
        "                 ('GBM', GradientBoostingClassifier(learning_rate = lr, loss = los, max_depth = dep,max_features = fea,n_estimators = n_est))])\n",
        "  gbmpipeline.fit(X_train,y_train)\n",
        "  gbm_pred = gbmpipeline.predict(X_test)\n",
        "  fscore = fbeta_score(gbm_pred,y_test,1)\n",
        "  fscore_gbm_list.append(fscore)\n",
        "\n",
        "#average fscore calculation\n",
        "import statistics \n",
        "print(fscore_gbm_list)\n",
        "print(\"gbm Fscore:\",statistics.mean(fscore_gbm_list))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   13.9s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   37.0s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  5.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   40.0s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1580 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  6.1min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   14.3s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   38.2s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  6.0min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.8431372549019608, 0.2807017543859649, 0.8468468468468469]\n",
            "gbm Fscore: 0.6568952853782575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t1JWlVfF33C",
        "outputId": "933b0b17-8de9-40e6-8c0c-3d40c05d587e"
      },
      "source": [
        "#random forest\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "f1 = make_scorer(f1_score,average='macro')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "def normalize(df):\n",
        "    result = df.copy()\n",
        "    for column in df.columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        result[column] = (df[column] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "\n",
        "test_data = train_data.iloc[320:,:]\n",
        "train_data = train_data.iloc[:320,:]\n",
        "test_labels = train_labels.iloc[320:,:]\n",
        "train_labels = train_labels.iloc[:320,:]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "fscore_rfc_list = []\n",
        "for i in range(0,3):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20) \n",
        "\n",
        "  X_train = X_train.iloc[:,1:]\n",
        "  X_test = X_test.iloc[:,1:]\n",
        "  y_train = y_train.iloc[:,1:]\n",
        "  y_test = y_test.iloc[:,1:]\n",
        "\n",
        "  #normalization\n",
        "  X_train = normalize(X_train)\n",
        "  X_test = normalize(X_test)\n",
        "\n",
        "\n",
        "  # ada boost grid search to find best parameters\n",
        "\n",
        "  rfc = RandomForestClassifier()\n",
        "\n",
        "  param_grid = { \n",
        "    'n_estimators': [5,20,50,100],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_samples_split' : list(np.linspace(0.1, 1.0, 10, endpoint=True)),\n",
        "    'max_depth' : list(np.linspace(1, 10, 10, endpoint=True)),\n",
        "    'criterion' : ['gini', 'entropy']}\n",
        "\n",
        "  pca = PCA().fit(X_train)\n",
        "  comp_list = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_comp = np.where(comp_list == max(np.cumsum(pca.explained_variance_ratio_)))\n",
        "  n_comp = int(n_comp[0])\n",
        "\n",
        "\n",
        "  CV_rfc = GridSearchCV(estimator = rfc, param_grid = param_grid , cv = 5, scoring = f1, n_jobs = -1, verbose = 2)\n",
        "  CV_rfc.fit(X_train, y_train)\n",
        "  CV_rfc.best_params_\n",
        "  n = CV_rfc.best_params_['n_estimators']\n",
        "  f = CV_rfc.best_params_['max_features']\n",
        "  sam = CV_rfc.best_params_['min_samples_split']\n",
        "  fd = CV_rfc.best_params_['max_depth']\n",
        "  cri = CV_rfc.best_params_['criterion']\n",
        "\n",
        "  rfcpipeline = Pipeline([('pca', PCA(n_components = n_comp)),\n",
        "                 ('RFC', RandomForestClassifier(n_estimators = n, max_features = f, min_samples_split = sam,max_depth = fd,criterion = cri))])\n",
        "  rfcpipeline.fit(X_train,y_train)\n",
        "  rfc_pred = rfcpipeline.predict(X_test)\n",
        "  fscore = fbeta_score(rfc_pred,y_test,1)\n",
        "  fscore_rfc_list.append(fscore)\n",
        "\n",
        "#average fscore calculation\n",
        "import statistics \n",
        "print(fscore_rfc_list)\n",
        "print(\"rfc Fscore:\",statistics.mean(fscore_rfc_list))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2400 candidates, totalling 12000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.8s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:   18.2s\n",
            "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=-1)]: Done 1256 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1986 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done 2876 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done 3930 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done 5144 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=-1)]: Done 6522 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done 8060 tasks      | elapsed:  8.5min\n",
            "[Parallel(n_jobs=-1)]: Done 9762 tasks      | elapsed: 10.4min\n",
            "[Parallel(n_jobs=-1)]: Done 11624 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=-1)]: Done 12000 out of 12000 | elapsed: 13.0min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2400 candidates, totalling 12000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:   35.8s\n",
            "[Parallel(n_jobs=-1)]: Done 1424 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 2556 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 4016 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done 5796 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=-1)]: Done 7904 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=-1)]: Done 10332 tasks      | elapsed: 10.8min\n",
            "[Parallel(n_jobs=-1)]: Done 12000 out of 12000 | elapsed: 12.7min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2400 candidates, totalling 12000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    7.4s\n",
            "[Parallel(n_jobs=-1)]: Done 612 tasks      | elapsed:   34.9s\n",
            "[Parallel(n_jobs=-1)]: Done 1424 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 2556 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 4016 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=-1)]: Done 5796 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=-1)]: Done 7904 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=-1)]: Done 10332 tasks      | elapsed: 10.8min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.8333333333333334, 0.617283950617284, 0.5]\n",
            "rfc Fscore: 0.6502057613168725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 12000 out of 12000 | elapsed: 12.7min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIKP8qcCVkWU",
        "outputId": "11b7920f-7661-4b88-8baf-744979e670fa"
      },
      "source": [
        "#prediciting labels for the test set with the models adaboost,svm,knn\n",
        "# test_labels = test_labels.iloc[:,1:]\n",
        "# test_data = test_data.iloc[:,:-1]\n",
        "\n",
        "#adaboost validation\n",
        "ada_pred = Adaboostpipeline.predict(test_features)\n",
        "ada_fscore = fbeta_score(ada_pred,test_class,1)\n",
        "print(\"fscore Adaboost\",ada_fscore)\n",
        "\n",
        "#knn validation\n",
        "knn_pred = knnpipeline.predict(test_features)\n",
        "knn_fscore = fbeta_score(knn_pred,test_class,1)\n",
        "print(\"fscore KNN\",knn_fscore)\n",
        "\n",
        "#svm validation\n",
        "svm_pred = svmpipeline.predict(test_features)\n",
        "svm_fscore = fbeta_score(svm_pred,test_class,1)\n",
        "print(\"fscore SVM\",svm_fscore)\n",
        "\n",
        "#random forest validation\n",
        "rfc_pred = rfcpipeline.predict(test_features)\n",
        "rfc_fscore = fbeta_score(rfc_pred,test_class,1)\n",
        "print(\"fscore Random Forest\",rfc_fscore)\n",
        "\n",
        "#gbm validation\n",
        "gbm_pred = gbmpipeline.predict(test_features)\n",
        "gbm_fscore = fbeta_score(gbm_pred,test_class,1)\n",
        "print(\"fscore Gradient Boosting\",gbm_fscore)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fscore Adaboost 0.7874015748031497\n",
            "fscore KNN 0.845360824742268\n",
            "fscore SVM 0.2711864406779661\n",
            "fscore Random Forest 0.7142857142857144\n",
            "fscore Gradient Boosting 0.5205479452054794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvY5efq0cpWj",
        "outputId": "ac5e5f71-c841-4bc4-dc20-3d707cfc14ce"
      },
      "source": [
        "#using the entire data to fit the model to prevent data loss before performing prediction for the original test_features.csv file\n",
        "\n",
        "train_data = pd.read_csv(\"train_features.csv\", header = None)\n",
        "train_labels = pd.read_csv(\"train_labels.csv\")\n",
        "submission_data = pd.read_csv(\"test_features.csv\",header = None)\n",
        "submission_features = submission_data.iloc[:,1:]\n",
        "submission_features = normalize(submission_features)\n",
        "print(submission_features)\n",
        "entire_feature_data = train_data.iloc[:,1:]\n",
        "entire_labels = train_labels.iloc[:,1:]\n",
        "\n",
        "#fitting  and predicting for adaboosst\n",
        "# Adaboostpipeline.fit(entire_feature_data,entire_labels)\n",
        "adaboost_prediction = Adaboostpipeline.predict(submission_features)\n",
        "#fitting and predicting for svm\n",
        "# svmpipeline.fit(entire_feature_data,entire_labels)\n",
        "svm_prediction = svmpipeline.predict(submission_features)\n",
        "#fitting and predicting for knn\n",
        "# knnpipeline.fit(entire_feature_data,entire_labels)\n",
        "knn_prediction = knnpipeline.predict(submission_features)\n",
        "#fitting and predicting for gbm\n",
        "# gbmpipeline.fit(entire_feature_data,entire_labels)\n",
        "gbm_prediction = gbmpipeline.predict(submission_features)\n",
        "#fitting and predicting for random forest\n",
        "# rfcpipeline.fit(entire_feature_data,entire_labels)\n",
        "rfc_prediction = rfcpipeline.predict(submission_features)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          1         2         3    ...       98        99        100\n",
            "0    0.159282  0.542787  0.144517  ...  0.410779  0.596365  0.420361\n",
            "1    0.209768  0.461960  0.117974  ...  0.488389  0.746800  0.400259\n",
            "2    0.085157  0.514534  0.142879  ...  0.512598  0.696311  0.238587\n",
            "3    0.141448  0.510710  0.136751  ...  0.378696  0.469460  0.364383\n",
            "4    0.171508  0.471559  0.124421  ...  0.405020  0.667990  0.395360\n",
            "..        ...       ...       ...  ...       ...       ...       ...\n",
            "391  0.258425  0.476906  0.116885  ...  0.418206  0.630979  0.318621\n",
            "392  0.085914  0.509588  0.141164  ...  0.318432  0.595942  0.404258\n",
            "393  0.106602  0.507377  0.138775  ...  0.510248  0.723575  0.355947\n",
            "394  0.092029  0.491673  0.135993  ...  0.497814  0.574008  0.559471\n",
            "395  0.083982  0.521568  0.144834  ...  0.441405  0.468004  0.419153\n",
            "\n",
            "[396 rows x 100 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEYJ2uTrkr2u"
      },
      "source": [
        "#writing predictions to a file in specified format\n",
        "import csv\n",
        "\n",
        "with open('adaboost_output.csv', 'w') as f:\n",
        "  f.write(\"%s,%s\\n\"%(\"id\", \"label\"))\n",
        "  for i in range(len(adaboost_prediction)):\n",
        "    f.write(\"%s,%s\\n\"%(i, adaboost_prediction[i]))\n",
        "\n",
        "with open('svm_output.csv', 'w') as f:\n",
        "  f.write(\"%s,%s\\n\"%(\"id\", \"label\"))\n",
        "  for i in range(len(svm_prediction)):\n",
        "    f.write(\"%s,%s\\n\"%(i, svm_prediction[i]))\n",
        "\n",
        "with open('knn_output.csv', 'w') as f:\n",
        "  f.write(\"%s,%s\\n\"%(\"id\", \"label\"))\n",
        "  for i in range(len(knn_prediction)):\n",
        "    f.write(\"%s,%s\\n\"%(i, knn_prediction[i]))\n",
        "\n",
        "with open('gbm_output.csv', 'w') as f:\n",
        "  f.write(\"%s,%s\\n\"%(\"id\", \"label\"))\n",
        "  for i in range(len(gbm_prediction)):\n",
        "    f.write(\"%s,%s\\n\"%(i, gbm_prediction[i]))\n",
        "    \n",
        "with open('rfc_output.csv', 'w') as f:\n",
        "  f.write(\"%s,%s\\n\"%(\"id\", \"label\"))\n",
        "  for i in range(len(rfc_prediction)):\n",
        "    f.write(\"%s,%s\\n\"%(i, rfc_prediction[i]))\n"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}